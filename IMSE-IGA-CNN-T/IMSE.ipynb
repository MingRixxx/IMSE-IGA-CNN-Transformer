{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:15:32.887199Z",
     "start_time": "2025-05-08T12:15:32.775197Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "file_names = ['0_0.mat','7_1.mat','7_2.mat','7_3.mat','14_1.mat','14_2.mat','14_3.mat','21_1.mat','21_2.mat','21_3.mat']\n",
    "\n",
    "for file in file_names:\n",
    "    data = loadmat(f'matfiles\\\\{file}')\n",
    "    print(list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:15:32.934198Z",
     "start_time": "2025-05-08T12:15:32.888198Z"
    }
   },
   "outputs": [],
   "source": [
    "data_columns = ['X097_DE_time', 'X105_DE_time', 'X118_DE_time', 'X130_DE_time', 'X169_DE_time',\n",
    "                'X185_DE_time','X197_DE_time','X209_DE_time','X222_DE_time','X234_DE_time']\n",
    "columns_name = ['de_normal','de_7_inner','de_7_ball','de_7_outer','de_14_inner','de_14_ball','de_14_outer','de_21_inner','de_21_ball','de_21_outer']\n",
    "data_12k_10c = pd.DataFrame()\n",
    "for index in range(10):\n",
    "    data = loadmat(f'matfiles\\\\{file_names[index]}')\n",
    "    dataList = data[data_columns[index]].reshape(-1)\n",
    "    data_12k_10c[columns_name[index]] = dataList[:119808]  \n",
    "print(data_12k_10c.shape)\n",
    "data_12k_10c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:15:33.920199Z",
     "start_time": "2025-05-08T12:15:32.934198Z"
    }
   },
   "outputs": [],
   "source": [
    "data_12k_10c.set_index('de_normal',inplace=True)\n",
    "data_12k_10c.to_csv('data_12k_10c.csv')\n",
    "data_12k_10c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:15:34.573198Z",
     "start_time": "2025-05-08T12:15:33.921199Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import KFold  \n",
    "import torch\n",
    "\n",
    "def split_data_with_overlap(data, time_steps, label, overlap_ratio=0.5):\n",
    "    stride = int(time_steps * (1 - overlap_ratio)) \n",
    "    samples = (len(data) - time_steps) // stride + 1  \n",
    "    data_list = []\n",
    "    for i in range(samples):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + time_steps\n",
    "        temp_data = data[start_idx:end_idx].tolist()\n",
    "        temp_data.append(label)  \n",
    "        data_list.append(temp_data)\n",
    "    return pd.DataFrame(data_list, columns=[x for x in range(time_steps + 1)])\n",
    "\n",
    "def normalize(data):\n",
    "    data = np.array(data)\n",
    "    s = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-8) \n",
    "    return s\n",
    "\n",
    "def make_datasets(data_file_csv, test_rate=0.1):\n",
    "    origin_data = pd.read_csv(data_file_csv)\n",
    "    time_steps = 1024  \n",
    "    overlap_ratio = 0.5  \n",
    "    all_samples = []\n",
    "    label = 0  \n",
    "\n",
    "    for column_name, column_data in origin_data.items():\n",
    "        column_data = normalize(column_data)  \n",
    "        split_data = split_data_with_overlap(column_data, time_steps, label, overlap_ratio)\n",
    "        label += 1\n",
    "        all_samples.append(split_data)\n",
    "\n",
    "    min_samples = min([len(sample) for sample in all_samples])\n",
    "    all_samples_align = [sample.iloc[:min_samples, :] for sample in all_samples]\n",
    "\n",
    "    total_data = pd.concat(all_samples_align, axis=0).reset_index(drop=True)\n",
    "    total_data = sklearn.utils.shuffle(total_data).reset_index(drop=True)\n",
    "\n",
    "    test_len = int(len(total_data) * test_rate)\n",
    "    test_set = total_data.iloc[:test_len, :]\n",
    "    kfold_data = total_data.iloc[test_len:, :]\n",
    "\n",
    "    return kfold_data, test_set\n",
    "\n",
    "def make_data_labels(dataframe):\n",
    "    x_data = dataframe.iloc[:, 0:-1]\n",
    "    y_label = dataframe.iloc[:, -1]\n",
    "    x_data = torch.tensor(x_data.values).float()\n",
    "    y_label = torch.tensor(y_label.values.astype('int64'))\n",
    "    x_data = x_data.unsqueeze(1)\n",
    "    \n",
    "    return x_data, y_label\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kfold_data, test_set = make_datasets('data_12k_10c.csv', test_rate=0.1)\n",
    "    test_x, test_y = make_data_labels(test_set)\n",
    "    dump(test_x, 'testX_1024_10c')\n",
    "    dump(test_y, 'testY_1024_10c')\n",
    "    print(f\"独立测试集 shape: X={test_x.shape}, Y={test_y.shape}\")\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)  \n",
    "    fold_num = 1\n",
    "\n",
    "    for train_index, val_index in kf.split(kfold_data):\n",
    "        print(f\"\\n===== 正在生成第 {fold_num} 折数据集 =====\")\n",
    "        train_fold = kfold_data.iloc[train_index, :]\n",
    "        val_fold = kfold_data.iloc[val_index, :]\n",
    "        \n",
    "        train_x, train_y = make_data_labels(train_fold)\n",
    "        val_x, val_y = make_data_labels(val_fold)\n",
    "        \n",
    "        dump(train_x, f'trainX_fold{fold_num}_1024_10c')\n",
    "        dump(train_y, f'trainY_fold{fold_num}_1024_10c')\n",
    "        dump(val_x, f'valX_fold{fold_num}_1024_10c')\n",
    "        dump(val_y, f'valY_fold{fold_num}_1024_10c')\n",
    "        \n",
    "        print(f\"第{fold_num}折 训练集 shape: X={train_x.shape}, Y={train_y.shape}\")\n",
    "        print(f\"第{fold_num}折 验证集 shape: X={val_x.shape}, Y={val_y.shape}\")\n",
    "        \n",
    "        fold_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-08T12:15:37.441198Z"
    },
    "is_executing": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.signal import hilbert\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "if not hasattr(np, 'find_common_type'):\n",
    "    def find_common_type(array_types, scalar_types):\n",
    "        return np.result_type(*array_types, *scalar_types)\n",
    "    np.find_common_type = find_common_type\n",
    "if not hasattr(np, 'chararray'):\n",
    "    class chararray(np.ndarray):\n",
    "        pass\n",
    "    np.chararray = chararray\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "try:\n",
    "    from PyEMD import EMD\n",
    "except ImportError:\n",
    "    from PyEMD.EMD import EMD\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "def svr_extension(signal, extend_num=50, C=1.0, gamma=0.1, epsilon=0.1):\n",
    "    N = len(signal)\n",
    "    left_train_t = np.arange(N).reshape(-1, 1)\n",
    "    left_train_x = signal[:N]\n",
    "    svr_left = SVR(C=C, gamma=gamma, epsilon=epsilon)\n",
    "    svr_left.fit(left_train_t, left_train_x)\n",
    "    left_ext = svr_left.predict(np.arange(-extend_num, 0).reshape(-1, 1))\n",
    "\n",
    "    right_train_t = np.arange(len(signal) - N, len(signal)).reshape(-1, 1)\n",
    "    right_train_x = signal[-N:]\n",
    "    svr_right = SVR(C=C, gamma=gamma, epsilon=epsilon)\n",
    "    svr_right.fit(right_train_t, right_train_x)\n",
    "    right_ext = svr_right.predict(np.arange(len(signal), len(signal) + extend_num).reshape(-1, 1))\n",
    "\n",
    "    return np.concatenate([left_ext, signal, right_ext])\n",
    "\n",
    "def calculate_RMS(data1, data2):\n",
    "    return np.sqrt(np.mean((data1 - data2)**2))\n",
    "\n",
    "def calculate_hilbert_spectrum_entropy(imfs):\n",
    "    total_entropy = 0\n",
    "    for imf in imfs:\n",
    "        analytic_signal = hilbert(imf)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        power_spectrum = np.abs(np.fft.fft(amplitude_envelope))**2\n",
    "        power_spectrum_normalized = power_spectrum / np.sum(power_spectrum)\n",
    "        entropy = -np.sum(power_spectrum_normalized * np.log2(power_spectrum_normalized + 1e-10))\n",
    "        total_entropy += entropy\n",
    "    return total_entropy / len(imfs) if len(imfs) > 0 else 0\n",
    "\n",
    "def evaluate_imfs(imfs, original):\n",
    "    original_length = len(original)\n",
    "    end_region_length = int(0.1 * original_length)\n",
    "    rmse_front = calculate_RMS(imfs[:, :end_region_length], original[:end_region_length])\n",
    "    rmse_back = calculate_RMS(imfs[:, -end_region_length:], original[-end_region_length:])\n",
    "    rmse = (rmse_front + rmse_back) / 2\n",
    "    hse = calculate_hilbert_spectrum_entropy(imfs)\n",
    "    return {'rmse': rmse, 'hse': hse}\n",
    "\n",
    "fitness_cache = {}\n",
    "\n",
    "def fitness_function(params, signal, extend_num=50):\n",
    "    params_tuple = tuple(params)\n",
    "    if params_tuple in fitness_cache:\n",
    "        return fitness_cache[params_tuple]\n",
    "    C, gamma, epsilon = params\n",
    "    x_svr = svr_extension(signal, extend_num, C, gamma, epsilon)\n",
    "    emd = EMD(max_imf=7, max_sifting=200, tol=1e-1, spline_kind='linear')\n",
    "    imfs_svr = emd.emd(x_svr)\n",
    "    if len(imfs_svr) == 0:\n",
    "        fitness = [9999,9999]\n",
    "        fitness_cache[params_tuple] = fitness\n",
    "        return fitness\n",
    "    imfs_svr_cropped = imfs_svr[:, extend_num:extend_num + len(signal)]\n",
    "    eval_result = evaluate_imfs(imfs_svr_cropped, signal)\n",
    "    fitness = [eval_result['rmse'], eval_result['hse']]\n",
    "    fitness_cache[params_tuple] = fitness\n",
    "    return fitness\n",
    "\n",
    "def non_dominated_sorting(population, signal, extend_num=50):\n",
    "    pop_size = len(population)\n",
    "    if pop_size == 0:\n",
    "        return [[0]]\n",
    "    fronts = []\n",
    "    S = [[] for _ in range(pop_size)]\n",
    "    n = [0] * pop_size\n",
    "    rank = [0] * pop_size\n",
    "    fitness_values = [fitness_function(p, signal, extend_num) for p in population]\n",
    "\n",
    "    for p in range(pop_size):\n",
    "        for q in range(pop_size):\n",
    "            if p != q:\n",
    "                p_dominates = all(fitness_values[p][i] <= fitness_values[q][i] for i in range(2)) and any(fitness_values[p][i] < fitness_values[q][i] for i in range(2))\n",
    "                if p_dominates:\n",
    "                    S[p].append(q)\n",
    "                else:\n",
    "                    n[p] += 1\n",
    "        if n[p] == 0:\n",
    "            rank[p] = 0\n",
    "            if not fronts: fronts.append([])\n",
    "            fronts[0].append(p)\n",
    "    i = 0\n",
    "    while i < len(fronts) and fronts[i]:\n",
    "        Q = []\n",
    "        for p in fronts[i]:\n",
    "            for q in S[p]:\n",
    "                n[q] -= 1\n",
    "                if n[q] == 0:\n",
    "                    rank[q] = i+1\n",
    "                    Q.append(q)\n",
    "        i +=1\n",
    "        if Q: fronts.append(Q)\n",
    "    if not fronts:\n",
    "        fronts = [[i for i in range(pop_size)]]\n",
    "    return fronts\n",
    "\n",
    "def calculate_crowding_distance(front, fitness_values):\n",
    "    front_len = len(front)\n",
    "    if front_len <= 1:\n",
    "        return [0.0] * front_len\n",
    "    \n",
    "    crowding_distances = [0.0] * front_len\n",
    "    for i in range(2):\n",
    "        if len(fitness_values) == 0:\n",
    "            continue\n",
    "        sorted_idx = sorted(front, key=lambda x: fitness_values[x][i])\n",
    "        crowding_distances[sorted_idx[0]] = float('inf')\n",
    "        crowding_distances[sorted_idx[-1]] = float('inf')\n",
    "        \n",
    "        f_max = fitness_values[sorted_idx[-1]][i]\n",
    "        f_min = fitness_values[sorted_idx[0]][i]\n",
    "        if f_max - f_min > 1e-6:\n",
    "            for j in range(1, front_len-1):\n",
    "                crowding_distances[sorted_idx[j]] += (fitness_values[sorted_idx[j+1]][i] - fitness_values[sorted_idx[j-1]][i])/(f_max-f_min)\n",
    "    return crowding_distances\n",
    "\n",
    "def quantum_mutation(coati, lower_bound, upper_bound):\n",
    "    num_dimensions = len(coati)\n",
    "    for i in range(num_dimensions):\n",
    "        if random.random() < 0.1:\n",
    "            delta = 0.5 * np.pi * (2*random.random()-1)\n",
    "            coati[i] = np.clip(coati[i] + delta*(upper_bound[i]-lower_bound[i]), lower_bound[i], upper_bound[i])\n",
    "    return coati\n",
    "\n",
    "def logistic_map(x0=0.2, r=3.9, num_points=10):\n",
    "    seq = [x0]\n",
    "    for _ in range(num_points-1): seq.append(r*seq[-1]*(1-seq[-1]))\n",
    "    return np.array(seq)\n",
    "\n",
    "def MOCOA(num_coatis=10, num_dimensions=3, num_generations=1, lower_bound=[0.1,0.01,0.01], upper_bound=[10,1,1], signal=None, extend_num=50):\n",
    "    lower_bound = np.array(lower_bound)\n",
    "    upper_bound = np.array(upper_bound)\n",
    "    population = np.zeros((num_coatis, num_dimensions))\n",
    "    for d in range(num_dimensions):\n",
    "        population[:,d] = lower_bound[d] + logistic_map(num_points=num_coatis)*(upper_bound[d]-lower_bound[d])\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        fronts = non_dominated_sorting(population, signal, extend_num)\n",
    "        if not fronts:\n",
    "            return population, []\n",
    "        fitness_values = [fitness_function(p, signal, extend_num) for p in population]\n",
    "        crowding_distances = []\n",
    "        for front in fronts: \n",
    "            cd = calculate_crowding_distance(front, fitness_values)\n",
    "            crowding_distances.extend(cd)\n",
    "        if len(fronts[0])>0 and len(crowding_distances)>0:\n",
    "            leader_idx = fronts[0][np.argmax(crowding_distances[:len(fronts[0])])]\n",
    "            leader = population[leader_idx]\n",
    "        else:\n",
    "            leader = population[0]\n",
    "\n",
    "        if generation < num_generations*0.7:\n",
    "            for i in range(num_coatis):\n",
    "                population[i] = np.clip(population[i] + (2*random.random()-1)*(leader-population[i]), lower_bound, upper_bound)\n",
    "        else:\n",
    "            for i in range(num_coatis):\n",
    "                population[i] = np.clip(population[i] + random.random()*0.1*(leader-population[i]), lower_bound, upper_bound)\n",
    "        \n",
    "        for i in range(num_coatis):\n",
    "            if random.random() <0.1: population[i] = quantum_mutation(population[i], lower_bound, upper_bound)\n",
    "\n",
    "    final_fronts = non_dominated_sorting(population, signal, extend_num)\n",
    "    if not final_fronts or len(final_fronts[0])==0:\n",
    "        return population, []\n",
    "    return [population[i] for i in final_fronts[0]], []\n",
    "\n",
    "def select_final_params(pareto_front, signal, extend_num=50):\n",
    "    if len(pareto_front) == 0:\n",
    "        return [2.0, 0.2, 0.1]\n",
    "    fitness_values = [fitness_function(p, signal, extend_num) for p in pareto_front]\n",
    "    return pareto_front[np.argmin([f[0] for f in fitness_values])]\n",
    "\n",
    "def imf_make_unify(data, labels, imfs_unify=7):\n",
    "    samples, _, signal_len = data.shape\n",
    "    emd_result = np.zeros((samples, imfs_unify, signal_len))\n",
    "    delete_list = []\n",
    "    for i in range(samples):\n",
    "        imf = data[i]\n",
    "        if imf.shape[0] == imfs_unify:\n",
    "            emd_result[i] = imf\n",
    "        elif imf.shape[0] > imfs_unify:\n",
    "            emd_result[i] = np.vstack([imf[:imfs_unify-1], np.sum(imf[imfs_unify-1:], axis=0)])\n",
    "        else:\n",
    "            delete_list.append(i)\n",
    "    for idx in sorted(delete_list, reverse=True):\n",
    "        emd_result = np.delete(emd_result, idx, axis=0)\n",
    "        labels = np.delete(labels, idx, axis=0)\n",
    "    return torch.tensor(emd_result, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "def unify_imf_components(imfs, target_num=7):\n",
    "    if imfs.size == 0 or imfs.ndim < 2:\n",
    "        return np.zeros((target_num, 1024))\n",
    "    current_num, sig_len = imfs.shape\n",
    "    if current_num == target_num: return imfs.copy()\n",
    "    elif current_num > target_num: return np.vstack([imf[:target_num-1] for imf in [imfs]] + [np.sum(imfs[target_num-1:], axis=0)])\n",
    "    else: return np.vstack([imfs, np.zeros((target_num-current_num, sig_len))])\n",
    "\n",
    "def process_emd_with_mocoa(test_xdata, test_ylabel, target_imfs=7, num_coatis=10, num_generations=1, extend_num=50):\n",
    "    global fitness_cache\n",
    "    fitness_cache.clear()\n",
    "    if isinstance(test_xdata, torch.Tensor):\n",
    "        test_xdata = test_xdata.numpy().squeeze(axis=1) if test_xdata.ndim==3 else test_xdata.numpy()\n",
    "    if isinstance(test_ylabel, torch.Tensor):\n",
    "        test_ylabel = test_ylabel.numpy()\n",
    "    \n",
    "    emd = EMD(max_imf=7, max_sifting=200, tol=1e-1, spline_kind='linear')\n",
    "    all_emd_results = []\n",
    "    len_data = len(test_xdata)\n",
    "    logging.info(f\"开始处理样本，共 {len_data} 个\")\n",
    "\n",
    "    for idx, signal in enumerate(test_xdata):\n",
    "        if idx % 20 == 0: logging.info(f\"进度: {idx}/{len_data}\")\n",
    "        pareto_front, _ = MOCOA(num_coatis=num_coatis, num_generations=num_generations, signal=signal, extend_num=extend_num)\n",
    "        best_params = select_final_params(pareto_front, signal, extend_num)\n",
    "\n",
    "        extended_signal = svr_extension(signal, extend_num, *best_params)\n",
    "        imfs = emd.emd(extended_signal)\n",
    "\n",
    "        cropped_imfs = imfs[:, extend_num:extend_num+len(signal)] if imfs.ndim>=2 else np.zeros((7, len(signal)))\n",
    "        unified_imfs = unify_imf_components(cropped_imfs, target_imfs)\n",
    "        all_emd_results.append(unified_imfs)\n",
    "    \n",
    "    all_emd_results = np.array(all_emd_results)\n",
    "    processed_x, processed_y = imf_make_unify(all_emd_results, test_ylabel, target_imfs)\n",
    "    logging.info(f\"处理完成，输出维度: {processed_x.shape}\")\n",
    "    return processed_x, processed_y\n",
    "\n",
    "def process_kfold_emd_mocoa(kfold_num=5, target_imfs=7, num_coatis=8, num_generations=1, extend_num=50):\n",
    "    for fold in range(1, kfold_num+1):\n",
    "        logging.info(\"=\"*50)\n",
    "        logging.info(f\"处理第 {fold} 折数据集\")\n",
    "        trainX = load(f'trainX_fold{fold}_1024_10c')\n",
    "        trainY = load(f'trainY_fold{fold}_1024_10c')\n",
    "        valX = load(f'valX_fold{fold}_1024_10c')\n",
    "        valY = load(f'valY_fold{fold}_1024_10c')\n",
    "        \n",
    "        train_x_emd, train_y_emd = process_emd_with_mocoa(trainX, trainY, target_imfs, num_coatis, num_generations, extend_num)\n",
    "        val_x_emd, val_y_emd = process_emd_with_mocoa(valX, valY, target_imfs, num_coatis, num_generations, extend_num)\n",
    "        \n",
    "        dump(train_x_emd, f'emd_trainX_fold{fold}_MOCOA_1024_10c')\n",
    "        dump(train_y_emd, f'emd_trainY_fold{fold}_MOCOA_1024_10c')\n",
    "        dump(val_x_emd, f'emd_valX_fold{fold}_MOCOA_1024_10c')\n",
    "        dump(val_y_emd, f'emd_valY_fold{fold}_MOCOA_1024_10c')\n",
    "        logging.info(f\"第 {fold} 折处理完成！\")\n",
    "\n",
    "    logging.info(\"=\"*50)\n",
    "    logging.info(\"处理测试集\")\n",
    "    testX = load('testX_1024_10c')\n",
    "    testY = load('testY_1024_10c')\n",
    "    test_x_emd, test_y_emd = process_emd_with_mocoa(testX, testY, target_imfs, num_coatis, num_generations, extend_num)\n",
    "    dump(test_x_emd, 'emd_testX_MOCOA_1024_10c')\n",
    "    dump(test_y_emd, 'emd_testY_MOCOA_1024_10c')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_kfold_emd_mocoa(\n",
    "        kfold_num=5,\n",
    "        target_imfs=7,\n",
    "        num_coatis=8,        \n",
    "        num_generations=1,  \n",
    "        extend_num=50       \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
